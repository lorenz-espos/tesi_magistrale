{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9433474,"sourceType":"datasetVersion","datasetId":5714638},{"sourceId":9565494,"sourceType":"datasetVersion","datasetId":5794702},{"sourceId":9914966,"sourceType":"datasetVersion","datasetId":5886463}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Initialization**","metadata":{}},{"cell_type":"code","source":"!pip install langchain huggingface_hub sentence-transformers transformers langchain-community faiss-cpu faiss-gpu langchain-huggingface torch\n!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **RAG + LLM for payload generation**\n## Text Splitter","metadata":{}},{"cell_type":"code","source":"import os\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\nfrom langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint\nfrom langchain.vectorstores import FAISS\nfrom langchain.text_splitter import MarkdownTextSplitter\nfrom langchain.schema import Document\nfrom IPython.display import Markdown, display\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport gc\n\n\n# Funzione per caricare i file .md dalla directory manualmente\ndef load_markdown_documents(directory: str):\n    documents = []\n    for filename in os.listdir(directory):\n        if filename.endswith(\".md\"):\n            filepath = os.path.join(directory, filename)\n            with open(filepath, \"r\", encoding=\"utf-8\") as file:\n                content = file.read()\n                documents.append(Document(page_content=content, metadata={\"source\": filename}))\n    return documents\n\ndef create_retriever(documents):\n    print(\"Inizio creazione del retriever...\")\n\n    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n    splitter = MarkdownTextSplitter(chunk_size=450, chunk_overlap=80)\n    split_docs = splitter.split_documents(documents)\n    print(f\"Numero di documenti : {len(split_docs)}\")\n\n    texts = [doc.page_content for doc in split_docs]\n    vector_store = FAISS.from_texts(texts, embeddings)\n    #restituisce i top 3\n    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})  \n\n\n    #from langchain.vectorstores import Annoy\n\n    #vector_store = Annoy.from_texts(texts, embeddings)\n    #retriever = vector_store.as_retriever()\n\n    print(\"Retriever creato con successo\")\n    return retriever\n\n\n#Caricamento del modello generativo\ndef create_huggingface_model_local():\n    global model\n    #model_name = \"EleutherAI/gpt-neo-1.3B\"\n    #model_name = \"KimByeongSu/gpt-neo-1.3B_LAMA_TREx_finetuning_MAGNET_same\"\n    #model_name = \"KimByeongSu/gpt-neo-2.7B_LAMA_TREx_finetuning_MAGNET\"\n    model_name =\"ricepaper/vi-gemma-2b-RAG\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    def generate_text(prompt, max_length_i=1350, temperature_i=0.95):\n        print(\"Avvio del modello per la generazione del testo...\")\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            outputs = model.generate(\n            inputs['input_ids'],\n            attention_mask=inputs['attention_mask'],  # Passa l'attenzione mask\n            max_length=max_length_i,            # Ridotto ulteriormente per evitare output lunghi e ripetitivi\n            do_sample=True,            # Campionamento abilitato per la varietà\n            pad_token_id=tokenizer.eos_token_id,\n            temperature=temperature_i,           # Migliora la creatività\n            top_k=150,                  # Aumentato per fornire più opzioni durante la generazione\n            top_p=0.95,                 # Probabilità cumulativa controllata\n            repetition_penalty=2.6,     # Penalizza ripetizioni eccessive di token\n            num_beams=5,                # Beam search per maggiore coerenza\n            early_stopping=True        # Ferma la generazione quando una condizione è soddisfatta\n            )\n            return tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generate_text\n\ndef create_rag_chain(retriever, generate_text_fn):\n    def rag_chain(query):\n        relevant_docs = retriever.invoke(query)\n\n        context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n        full_prompt = f\"Context:{context}\\n\\n Question: {query} \\n\\n Your response: \"\n        \n        generated_text = generate_text_fn(full_prompt)\n        return generated_text, relevant_docs\n    \n    return rag_chain\n\n# Funzione per fare una query al sistema RAG\ndef query_rag(chain, query):\n    result, source_documents = chain(query)\n    \n    return result, source_documents\n\n# Funzione per svuotare la RAM della GPU\ndef optimizer_gpu():\n        model.to(\"cpu\")\n        print(\"Memoria dopo la generazione\")\n        print(f\"Memory allocated: {torch.cuda.memory_allocated()}\")\n        print(f\"Memory reserved: {torch.cuda.memory_reserved()}\")\n        torch.cuda.empty_cache()\n        gc.collect()\n        print(\"Memoria dopo l'ottimizzazione\")\n        print(f\"Memory allocated: {torch.cuda.memory_allocated()}\")\n        print(f\"Memory reserved: {torch.cuda.memory_reserved()}\")\n        model.to(\"cuda\")\n        \n\n# Esecuzione del sistema\nwhile True:\n    if __name__ == \"__main__\":\n        #IMPORTAZIONE DEL DATASET\n        #directory = \"/kaggle/input/docs-tools\"\n        directory=\"/kaggle/input/docs-tools-filtrato-2\"\n\n        documents = load_markdown_documents(directory)\n        retriever = create_retriever(documents)\n\n        # Usa il modello locale invece dell'endpoint remoto\n        generate_text_fn = create_huggingface_model_local()\n\n        rag_chain = create_rag_chain(retriever, generate_text_fn)\n\n        query = input(\"Inserisci una query :\")\n        response, docs = query_rag(rag_chain, query)\n\n        print(\"Risposta generata: \\n\", response)\n\n        print(f\"Numero di documenti utilizzati: {len(docs)}\")\n        j=0\n\n        for doc in docs:\n            j=j+1\n            print(f\"\\nDocumento sorgente {j}:\", doc.page_content)\n               \n\n            \n        \n        #Visualizza in markdown\n        i=int(input(\"Digita un numero maggiore di 0 per visualizzare la risposta in markdown\"))\n        if(i>0):\n                print(\"         =================================================\")\n                display(Markdown(response))\n                i=0\n                i=int(input(\"Vuoi salvare l'output come file markdown? In tal caso digita un numero maggiore di 0\"))\n                if(i>0):\n                    # Salvataggio del contenuto nel file Markdown\n                    filename=input(\"Inserisci il nome del file\")\n                    with open(filename, \"w\") as file:\n                        file.write(response)\n        i=0\n        optimizer_gpu()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}